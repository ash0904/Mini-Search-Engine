{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import xml.etree.ElementTree as ET\n",
    "import re                                                           \n",
    "from collections import defaultdict\n",
    "from Stemmer import Stemmer\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from heapq import heappush, heappop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(text):\n",
    "    words = re.split(r'(\\b[^-\\s]+\\b)((?<=\\.\\w).)?', text)\n",
    "    tok = [i for i in words if i!=None and i != \" \" and i != \"\"]\n",
    "    tok = [ word.lower() for word in tok if re.match('^[a-zA-Z0-9\\'-.]+$',word) and not re.match('^[0-9\\',-_]+$',word) and not re.match('^[^\\w]+$',word)]\n",
    "    return tok\n",
    "\n",
    "ps = Stemmer(\"english\")\n",
    "\n",
    "# ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def strip_tag_name(t):\n",
    "    t = elem.tag\n",
    "    idx = k = t.rfind(\"}\")\n",
    "    if idx != -1:\n",
    "        t = t[idx + 1:]\n",
    "    return t\n",
    "\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "def stopWords(listOfWords):                                         #Stop Words Removal\n",
    "    temp=[key for key in listOfWords if key not in stop_words]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findInfoBoxTextCategory(data):                                       \n",
    "    info=[]\n",
    "    bodyText=[]\n",
    "    category=[]\n",
    "    links=[]\n",
    "    flagtext=1\n",
    "    lines = data.split('\\n')\n",
    "    for i in xrange(len(lines)):\n",
    "        if '{{infobox' in lines[i]:\n",
    "            flag=0\n",
    "            temp=lines[i].split('{{infobox')[1:]\n",
    "            info.extend(temp)\n",
    "            while True:\n",
    "                if '{{' in lines[i]:\n",
    "                    count=lines[i].count('{{')\n",
    "                    flag+=count\n",
    "                if '}}' in lines[i]:\n",
    "                    count=lines[i].count('}}')\n",
    "                    flag-=count\n",
    "                if flag<=0:\n",
    "                    break\n",
    "                i+=1\n",
    "                info.append(lines[i])\n",
    "\n",
    "        elif flagtext:\n",
    "            if '[[category' in lines[i] or '==external links==' in lines[i]:\n",
    "                flagtext=0\n",
    "            bodyText.append(lines[i])\n",
    "            \n",
    "    else:\n",
    "        if \"[[category\" in lines[i]:\n",
    "            line = data.split(\"[[category:\")\n",
    "            if len(line)>1:\n",
    "                category.extend(line[1:-1])\n",
    "                temp=line[-1].split(']]')\n",
    "                category.append(temp[0])\n",
    "\n",
    "    category = my_tokenizer(' '.join(category))\n",
    "    category = stopWords(category)\n",
    "    category = map(ps.stemWord, category)\n",
    "\n",
    "    info = my_tokenizer(' '.join(info))\n",
    "    info = stopWords(info)\n",
    "    info = map(ps.stemWord,info)\n",
    "\n",
    "    bodyText = my_tokenizer(' '.join(bodyText))\n",
    "    bodyText = stopWords(bodyText)\n",
    "    bodyText = map(ps.stemWord, bodyText)\n",
    "\n",
    "    infobox = defaultdict(int)\n",
    "    for key in info:\n",
    "        infobox[key] += 1\n",
    "\n",
    "    bodyTxt = defaultdict(int)\n",
    "    for key in bodyText:\n",
    "        bodyTxt[key] += 1\n",
    "\n",
    "    categ = defaultdict(int)\n",
    "    for key in category:\n",
    "        categ[key] += 1\n",
    "  \n",
    "    return infobox, bodyTxt, categ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(title, text):\n",
    "    twords = my_tokenizer(title)\n",
    "    twords = stopWords(twords)\n",
    "    twords = map(ps.stemWord, twords)\n",
    "    \n",
    "    ttokens = defaultdict(int)\n",
    "    for key in twords:\n",
    "        ttokens[key]+=1\n",
    "    info, bodyText, category = findInfoBoxTextCategory(text)\n",
    "    return ttokens, bodyText, info, category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n",
      "65000\n",
      "70000\n",
      "75000\n",
      "80000\n",
      "85000\n",
      "90000\n",
      "95000\n",
      "100000\n",
      "105000\n",
      "110000\n",
      "115000\n",
      "120000\n",
      "125000\n",
      "130000\n",
      "135000\n",
      "Elapsed time: 0:31:59.89\n"
     ]
    }
   ],
   "source": [
    "dumpPath = 'smallLarge.xml'\n",
    "bow = defaultdict(str)\n",
    "# dumpPath = 'test.xml'\n",
    "pagesPerFile = 5000\n",
    "lastCount = pagesPerFile - 1\n",
    "start_time = time.time()\n",
    "totalCount = 0\n",
    "inver = defaultdict(str)\n",
    "for event, elem in ET.iterparse(dumpPath, events=('start', 'end')):\n",
    "    tname = strip_tag_name(elem.tag)\n",
    "    if event == 'start':\n",
    "        if tname == 'page':\n",
    "            title = ''\n",
    "            did = -1\n",
    "            redirect = ''\n",
    "            inrevision = False\n",
    "            ns = 0\n",
    "            text = ''\n",
    "        elif tname == 'revision':\n",
    "            # Do not pick up on revision id's\n",
    "            inrevision = True\n",
    "    else:\n",
    "        if tname == 'title':\n",
    "            title = elem.text\n",
    "        elif tname == 'id' and not inrevision:\n",
    "            did = int(elem.text)\n",
    "        elif tname == 'redirect':\n",
    "            redirect = elem.attrib['title']\n",
    "        elif tname == 'ns':\n",
    "            ns = int(elem.text)\n",
    "        elif tname == 'text':\n",
    "            text = elem.text\n",
    "        elif tname == 'page':\n",
    "            if redirect == \"\":\n",
    "                redirect = title\n",
    "            ttoken, body, tinfo, tcat = create_index(redirect,text)\n",
    "            for keys in set(ttoken.keys() + body.keys() + tinfo.keys()):\n",
    "                inver[keys] += \"|\" + str(did)\n",
    "                if keys in ttoken:\n",
    "                    inver[keys] += \"t\" + str(ttoken[keys])\n",
    "                if keys in body:\n",
    "                    inver[keys] += \"b\" + str(body[keys])\n",
    "                if keys in tinfo:\n",
    "                    inver[keys] += \"i\" + str(tinfo[keys])\n",
    "                if keys in tcat:\n",
    "                    inver[keys] += \"c\" + str(tcat[keys])\n",
    "            totalCount += 1\n",
    "        elem.clear()\n",
    "    if totalCount>lastCount and totalCount%pagesPerFile == 0:\n",
    "        print totalCount\n",
    "        lastCount = totalCount\n",
    "        fname = \"index/\" + str(totalCount/pagesPerFile) + \".txt\"\n",
    "        with open(fname, 'w') as fil:\n",
    "            for key in sorted(inver):\n",
    "                bow[key] += str(totalCount/pagesPerFile)\n",
    "                fil.write(key.encode('ascii', 'ignore').decode('ascii') + inver[key] + \"\\n\")\n",
    "        inver.clear()\n",
    "        inver = defaultdict(str)\n",
    "        gc.collect()\n",
    "        \n",
    "fname = \"index/\" + str(totalCount/pagesPerFile + 1) + \".txt\"\n",
    "with open(fname, 'w') as fil:\n",
    "    for key in sorted(inver):\n",
    "        bow[key] += str(totalCount/pagesPerFile)\n",
    "        fil.write(key.encode('ascii', 'ignore').decode('ascii') + inver[key] + \"\\n\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heap = []\n",
    "files = [ \"index/\" + f for f in sorted(os.listdir(\"index/\")) ]\n",
    "fp = [open(fil) for fil in files]\n",
    "\n",
    "def pushin(did):\n",
    "    pline = fp[did].readline().strip(\" \\n\")\n",
    "    if pline:\n",
    "        pline = list(pline.partition(\"|\"))\n",
    "        pline.append(did)\n",
    "        heappush(heap,pline)\n",
    "    else:\n",
    "        print did, \" ends\"\n",
    "    \n",
    "\n",
    "def nextTok():\n",
    "    cur = heappop(heap)\n",
    "    tok = cur[0]\n",
    "    plist = cur[2]\n",
    "    did = cur[3]\n",
    "    return tok, plist, did\n",
    "\n",
    "for did in xrange(len(fp)):\n",
    "    pushin(did)\n",
    "\n",
    "# index = defaultdict(str)\n",
    "cur, plist, did = nextTok()\n",
    "pushin(did)\n",
    "\n",
    "nword, nplist, did = nextTok()\n",
    "pushin(did)\n",
    "\n",
    "tot = 0\n",
    "cnt = 0\n",
    "fil = open('invertedIndex.txt', 'w')\n",
    "last = 'a'\n",
    "while heap:\n",
    "    if nword == cur:\n",
    "        plist += \"|\" + nplist\n",
    "        tot+=1\n",
    "    else:\n",
    "        fil.write(key.encode('ascii', 'ignore').decode('ascii') + \"|\" + index[key] + \"\\n\")\n",
    "#         index[cur] = plist\n",
    "        cur, plist = nword, nplist\n",
    "        if 'a' <= cur[0] <= 'z' and cur[:2] != last:\n",
    "            last = cur[:2]\n",
    "            print last\n",
    "#             for key in sorted(index):\n",
    "#                 cnt += 1\n",
    "                \n",
    "#             index.clear()\n",
    "#             index = defaultdict(str)\n",
    "#             gc.collect()\n",
    "            \n",
    "    nword, nplist, did = nextTok()\n",
    "    pushin(did)\n",
    "fp = [f.close() for f in fp]\n",
    "fil.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil = open('invertedIndex.txt', 'a')\n",
    "for key in sorted(index):\n",
    "    cnt += 1\n",
    "    fil.write(key.encode('ascii', 'ignore').decode('ascii') + \"|\" + index[key] + \"\\n\")\n",
    "fil.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
