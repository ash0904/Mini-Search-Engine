{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import xml.etree.ElementTree as ET\n",
    "import re                                                           \n",
    "from collections import defaultdict\n",
    "from Stemmer import Stemmer\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from heapq import heappush, heappop\n",
    "import sys\n",
    "from math import log10\n",
    "# from pympler.asizeof import asizeof\n",
    "import xml.sax\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = Stemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopWords(listOfWords):                                         #Stop Words Removal\n",
    "    temp=[key for key in listOfWords if key not in stop_words]\n",
    "    return temp\n",
    "\n",
    "\n",
    "def myTokenizer(text):\n",
    "    words = re.split(r'(\\b[^-\\s]+\\b)((?<=\\.\\w).)?', text)\n",
    "    tok = [i for i in words if i!=None and i != \" \" and i != \"\"]\n",
    "    tok = [ word.lower() for word in tok if re.match('^[a-zA-Z0-9\\'-.]+$',word) and not re.match('^[\\',-_]+$',word) and not re.match('^[^\\w]+$',word)]\n",
    "    fin_tok = []\n",
    "    for t in tok:\n",
    "        fin_tok.append(re.sub(\"[\\+*=&$@/(),.\\-!?:]+\", '', t))\n",
    "    fin_tok = [i for i in fin_tok if i!=None and i != \" \" and i != \"\"]\n",
    "    return fin_tok\n",
    "\n",
    "def processText(text):\n",
    "    # tokenization\n",
    "    fin_tok = myTokenizer(text)\n",
    "    \n",
    "    # StopWord removal \n",
    "    fin_tok = stopWords(fin_tok)\n",
    "    \n",
    "    #Stemming\n",
    "    fin_tok = map(ps.stemWord, fin_tok)\n",
    "    \n",
    "    # Final processing\n",
    "    tok = []\n",
    "    for t in fin_tok:\n",
    "        tok.append(re.sub(\"[']+\", '', t))\n",
    "    return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_tag_name(t):\n",
    "    t = elem.tag\n",
    "    idx = k = t.rfind(\"}\")\n",
    "    if idx != -1:\n",
    "        t = t[idx + 1:]\n",
    "    return t\n",
    "\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findExternalLinks(data):\n",
    "    if not data:\n",
    "        return defaultdict(int)\n",
    "    links = []\n",
    "    lines = data.split(\"==external links==\")\n",
    "    if len(lines) > 1:\n",
    "        lines = lines[1].split(\"\\n\")\n",
    "        for i in xrange(len(lines)):\n",
    "            if '* [' in lines[i] or '*[' in lines[i]:\n",
    "                word = \"\"\n",
    "                temp = lines[i].split(' ')\n",
    "                word = [key for key in temp if 'http' not in temp]\n",
    "                word = ' '.join(word).encode('utf-8')\n",
    "                links.append(word)\n",
    "    links = processText(' '.join(links))\n",
    "\n",
    "    temp = defaultdict(int)\n",
    "    for key in links:\n",
    "        temp[key] += 1\n",
    "    links = temp\n",
    "    return links\n",
    "\n",
    "def findReferences(linestr):\n",
    "    searchStr=linestr.split(\"==References==\")[1:]\n",
    "    if len(searchStr)<=1:\n",
    "        return defaultdict(int)\n",
    "    endInd=searchStr[1].find(\"==External Links==\")\n",
    "    if endInd==-1:\n",
    "        endInd=searchStr[1].find(\"[[Category:\")\n",
    "        if endInd==-1:\n",
    "            endInd=len(searchStr[1])\n",
    "    ref = defaultdict(int)\n",
    "    refs = processText(searchStr[1][:endInd])\n",
    "    for key in refs:\n",
    "        ref[key] += 1\n",
    "    return ref\n",
    "\n",
    "def findInfoBoxTextCategory(data):     \n",
    "    if not data:\n",
    "        return defaultdict(int), defaultdict(int), defaultdict(int)\n",
    "    info = []\n",
    "    bodyText = []\n",
    "    category = []\n",
    "    links = []\n",
    "    flagtext = 1\n",
    "    lines = data.split('\\n')\n",
    "    for i in xrange(len(lines)):\n",
    "        if '{{infobox' in lines[i]:\n",
    "            flag = 0\n",
    "            temp = lines[i].split('{{infobox')[1:]\n",
    "            info.extend(temp)\n",
    "            while True:\n",
    "                if '{{' in lines[i]:\n",
    "                    count = lines[i].count('{{')\n",
    "                    flag += count\n",
    "                if '}}' in lines[i]:\n",
    "                    count = lines[i].count('}}')\n",
    "                    flag -= count\n",
    "                if flag <= 0:\n",
    "                    break\n",
    "                i += 1\n",
    "                info.append(lines[i])\n",
    "\n",
    "        elif flagtext:\n",
    "            if '[[category' in lines[i] or '==external links==' in lines[i]:\n",
    "                flagtext=0\n",
    "            bodyText.append(lines[i])\n",
    "            \n",
    "    else:\n",
    "        if \"[[category\" in lines[i]:\n",
    "            line = data.split(\"[[category:\")\n",
    "            if len(line)>1:\n",
    "                category.extend(line[1:-1])\n",
    "                temp=line[-1].split(']]')\n",
    "                category.append(temp[0])\n",
    "\n",
    "    category = processText(' '.join(category))\n",
    "    info = processText(' '.join(info))\n",
    "    bodyText = processText(' '.join(bodyText))\n",
    "\n",
    "    infobox = defaultdict(int)\n",
    "    for key in info:\n",
    "        infobox[key] += 1\n",
    "\n",
    "    bodyTxt = defaultdict(int)\n",
    "    for key in bodyText:\n",
    "        bodyTxt[key] += 1\n",
    "\n",
    "    categ = defaultdict(int)\n",
    "    for key in category:\n",
    "        categ[key] += 1\n",
    "  \n",
    "    return infobox, bodyTxt, categ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(title, data):\n",
    "    twords = processText(title.lower())\n",
    "    extLinks = findExternalLinks(data.lower())\n",
    "    refer = findReferences(data.lower())\n",
    "    if data:\n",
    "        text = data.replace('_',' ').replace(',','')\n",
    "    \n",
    "    ttokens = defaultdict(int)\n",
    "    for key in twords:\n",
    "        ttokens[key]+=1\n",
    "    info, bodyText, category = findInfoBoxTextCategory(text)\n",
    "    return ttokens, bodyText, info, category, extLinks, refer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dumpPath = 'smallLarge.xml'\n",
    "# # dumpPath = 'wiki-search-small.xml'\n",
    "# dumpPath = 'enwiki-latest-pages-articles-multistream.xml'\n",
    "# pagesPerFile = 18000\n",
    "# lastCount = 1\n",
    "# start_time = time.time()\n",
    "# totalCount = 0\n",
    "# fileno = 1\n",
    "\n",
    "# inver = defaultdict(lambda: defaultdict(str))\n",
    "# ndoc  = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# titMap = open(\"titleMap.txt\",\"w\")\n",
    "\n",
    "# for event, elem in ET.iterparse(dumpPath, events=('start', 'end')):\n",
    "#     try:\n",
    "#         tname = strip_tag_name(elem.tag)\n",
    "#         if event == 'start':\n",
    "#             if tname == 'page':\n",
    "#                 title = ''\n",
    "#                 did = -1\n",
    "#                 redirect = ''\n",
    "#                 inrevision = False\n",
    "#                 text = ''\n",
    "#             elif tname == 'revision':\n",
    "#                 # Do not pick up on revision id's\n",
    "#                 inrevision = True\n",
    "#         else:\n",
    "#             if tname == 'title':\n",
    "#                 title = elem.text\n",
    "#             elif tname == 'id' and not inrevision:\n",
    "#                 did = int(elem.text)\n",
    "#             elif tname == 'redirect':\n",
    "#                 redirect = elem.attrib['title']\n",
    "#             elif tname == 'text':\n",
    "#                 text = elem.text\n",
    "#             elif tname == 'page':\n",
    "#                 totalCount += 1\n",
    "#                 if totalCount%8000 == 0:\n",
    "#                     print totalCount\n",
    "# #                 if totalCount <= 340003:\n",
    "# #                     elem.clear()\n",
    "# #                     continue\n",
    "#                 if redirect == \"\":\n",
    "#                     redirect = title\n",
    "#                 titMap.write(str(did) + \":\" + redirect.encode('ascii', 'ignore').decode('ascii')+ \"\\n\")\n",
    "#                 ttoken, body, tinfo, tcat, extLinks = create_index(redirect,text)\n",
    "#     #             vocab = list(set(ttoken.keys() + body.keys() + tinfo.keys() + extLinks.keys()))\n",
    "#     #             inver[keys][\"t\"] += \"|\" + str(did)\n",
    "\n",
    "#                 for keys in ttoken:\n",
    "#                     inver[\"t\"][keys] += \"|\" + str(did) + \":\" + str(ttoken[keys])\n",
    "#                     ndoc[\"t\"][keys]  += 1\n",
    "#                 for keys in body:\n",
    "#                     inver[\"b\"][keys] += \"|\" + str(did) + \":\" + str(body[keys])\n",
    "#                     ndoc[\"b\"][keys]  += 1\n",
    "#                 for keys in tinfo:\n",
    "#                     inver[\"i\"][keys] += \"|\" + str(did) + \":\" + str(tinfo[keys])\n",
    "#                     ndoc[\"i\"][keys]  += 1\n",
    "#                 for keys in tcat:\n",
    "#                     inver[\"c\"][keys] += \"|\" + str(did) + \":\" + str(tcat[keys])\n",
    "#                     ndoc[\"c\"][keys]  += 1\n",
    "#                 for keys in extLinks:\n",
    "#                     inver[\"e\"][keys] += \"|\" + str(did) + \":\" + str(extLinks[keys])\n",
    "#                     ndoc[\"e\"][keys]  += 1\n",
    "#             elem.clear()\n",
    "\n",
    "#         if totalCount>lastCount and totalCount%pagesPerFile == 0:\n",
    "#             lastCount = totalCount\n",
    "#             print fileno, \" pages -> \", totalCount\n",
    "#             for cat in \"tbice\":\n",
    "#                 fname = \"index/\" + cat + \"/\" + str(fileno) + \".txt\"\n",
    "#                 with open(fname, 'w') as fil:\n",
    "#                     for key in sorted(inver[cat]):\n",
    "#                         fil.write(key.encode('ascii', 'ignore').decode('ascii') + inver[cat][key] + \"\\n\")\n",
    "#             fileno += 1\n",
    "#             inver.clear()\n",
    "#             inver = defaultdict(lambda: defaultdict(str))\n",
    "#             gc.collect()\n",
    "#     except:\n",
    "#         continue\n",
    "        \n",
    "# for cat in \"tbice\":\n",
    "#     fname = \"index/\" + cat + \"/\" + str(fileno) + \".txt\"\n",
    "#     with open(fname, 'w') as fil:\n",
    "#         for key in sorted(inver[cat]):\n",
    "#             fil.write(key.encode('ascii', 'ignore').decode('ascii') + inver[cat][key] + \"\\n\")\n",
    "\n",
    "# elapsed_time = time.time() - start_time\n",
    "# print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  pages ->  1000\n",
      "2  pages ->  2000\n",
      "3  pages ->  3000\n",
      "4  pages ->  4000\n",
      "5  pages ->  5000\n"
     ]
    }
   ],
   "source": [
    "# dumpPath = 'smallLarge.xml'\n",
    "# dumpPath = 'wiki-search-small.xml'\n",
    "dumpPath = 'enwiki-latest-pages-articles-multistream.xml'\n",
    "pagesPerFile = 10000\n",
    "lastCount = 1\n",
    "start_time = time.time()\n",
    "totalCount = 0\n",
    "fileno = 1\n",
    "\n",
    "inver = defaultdict(lambda: defaultdict(str))\n",
    "ndoc  = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "titMap = open(\"titleMap.txt\",\"w\")\n",
    "\n",
    "class saxHandler( xml.sax.ContentHandler ):\n",
    "    global pageSize\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.CurrentData = \"\"\n",
    "        self.titleFlag = 0\n",
    "        self.textFlag = 0\n",
    "        self.idFlag=0\n",
    "        self.cnt=0\n",
    "        self.newsitems=[]\n",
    "        self.news={}\n",
    "\n",
    "\n",
    "    # Call when an element starts\n",
    "    def startElement(self, tag, attributes):\n",
    "        self.CurrentData = tag\n",
    "        if tag == 'title':\n",
    "            self.titleFlag = 1\n",
    "            self.titleBuff = \"\"\n",
    "        elif tag == 'redirect':\n",
    "            self.titleBuffer = attributes[\"title\"]\n",
    "        elif tag == 'text':\n",
    "            self.textFlag = 1\n",
    "            self.textBuffer = \"\"\n",
    "        elif tag == 'id':\n",
    "            self.idFlag = 1\n",
    "            self.idBuffer = \"\"\n",
    "\n",
    "\n",
    "   # Call when an elements ends\n",
    "    def endElement(self, tag):\n",
    "        global inver\n",
    "        global ndoc\n",
    "        global fileno\n",
    "        if tag == 'title':\n",
    "            self.titleFlag=0\n",
    "        if tag=='text' and self.textBuffer:\n",
    "            if self.titleBuffer == \"\":\n",
    "                self.titleBuffer = self.titleBuff\n",
    "            \n",
    "            titMap.write(str(self.did) + \":\" + self.titleBuffer.encode('ascii', 'ignore').decode('ascii')+ \"\\n\")\n",
    "            ttoken, body, tinfo, tcat, extLinks, refer = create_index(self.titleBuffer, self.textBuffer)\n",
    "            for keys in ttoken:\n",
    "                inver[\"t\"][keys] += \"|\" + str(self.did) + \":\" + str(ttoken[keys])\n",
    "                ndoc[\"t\"][keys]  += 1\n",
    "            for keys in body:\n",
    "                inver[\"b\"][keys] += \"|\" + str(self.did) + \":\" + str(body[keys])\n",
    "                ndoc[\"b\"][keys]  += 1\n",
    "            for keys in tinfo:\n",
    "                inver[\"i\"][keys] += \"|\" + str(self.did) + \":\" + str(tinfo[keys])\n",
    "                ndoc[\"i\"][keys]  += 1\n",
    "            for keys in tcat:\n",
    "                inver[\"c\"][keys] += \"|\" + str(self.did) + \":\" + str(tcat[keys])\n",
    "                ndoc[\"c\"][keys]  += 1\n",
    "            for keys in extLinks:\n",
    "                inver[\"e\"][keys] += \"|\" + str(self.did) + \":\" + str(extLinks[keys])\n",
    "                ndoc[\"e\"][keys]  += 1\n",
    "            for keys in refer:\n",
    "                inver[\"r\"][keys] += \"|\" + str(self.did) + \":\" + str(refer[keys])\n",
    "                ndoc[\"r\"][keys]  += 1\n",
    "            \n",
    "            self.textFlag=0\n",
    "        if tag=='id' and self.idBuffer and self.idFlag==1:\n",
    "            self.did = int(self.idBuffer)\n",
    "            self.idFlag+=1\n",
    "        if tag=='page':\n",
    "            self.idFlag=0\n",
    "            self.textFlag=0\n",
    "            self.titleFlag=0\n",
    "            self.cnt+=1\n",
    "            \n",
    "            if self.cnt%pagesPerFile == 0:\n",
    "                print fileno, \" pages -> \", self.cnt\n",
    "                for cat in \"tbicer\":\n",
    "                    fname = \"index/\" + cat + \"/\" + str(fileno) + \".txt\"\n",
    "                    with open(fname, 'w') as fil:\n",
    "                        for key in sorted(inver[cat]):\n",
    "                            fil.write(key.encode('ascii', 'ignore').decode('ascii') + inver[cat][key] + \"\\n\")\n",
    "                fileno += 1\n",
    "                inver.clear()\n",
    "                del inver\n",
    "                inver = defaultdict(lambda: defaultdict(str))\n",
    "                gc.collect()\n",
    "                \n",
    "        if tag==\"mediawiki\":\n",
    "            for cat in \"tbicer\":\n",
    "                fname = \"index/\" + cat + \"/\" + str(fileno) + \".txt\"\n",
    "                with open(fname, 'w') as fil:\n",
    "                    for key in sorted(inver[cat]):\n",
    "                        fil.write(key.encode('ascii', 'ignore').decode('ascii') + inver[cat][key] + \"\\n\")\n",
    "        self.CurrentData = \"\"\n",
    "\n",
    "\n",
    "   # Call when a character is read\n",
    "    def characters(self, content):\n",
    "        if self.titleFlag:\n",
    "            self.titleBuff += content\n",
    "        elif self.textFlag:\n",
    "            self.textBuffer += content\n",
    "        elif self.idFlag == 1:\n",
    "            self.idBuffer += content\n",
    "\n",
    "#New Index\n",
    "# create an XMLReader\n",
    "parser = xml.sax.make_parser()\n",
    "# turn off namepsaces\n",
    "parser.setFeature(xml.sax.handler.feature_namespaces, 0)\n",
    "# override the default ContextHandler\n",
    "Handler = saxHandler()\n",
    "parser.setContentHandler( Handler )\n",
    "parser.parse(dumpPath)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for mutiway merge sort starts\n",
    "\n",
    "\n",
    "heap = []\n",
    "fp = []\n",
    "def pushin(did):\n",
    "    pline = fp[did].readline().strip(\" \\n\")\n",
    "    if pline:\n",
    "        pline = list(pline.partition(\"|\"))\n",
    "        pline.append(did)\n",
    "        heappush(heap,pline)\n",
    "    else:\n",
    "        print did, \" ends\"\n",
    "    \n",
    "\n",
    "def nextTok():\n",
    "    cur = heappop(heap)\n",
    "    tok = cur[0]\n",
    "    plist = cur[2]\n",
    "    did = cur[3]\n",
    "    return tok, plist, did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<open file 'final_index/t.txt', mode 'r' at 0x7f370b1a1f60>, <open file 'final_index/t1.txt', mode 'r' at 0x7f370b1a16f0>]\n",
      "b\n",
      "c\n",
      "d\n",
      "e\n",
      "f\n",
      "g\n",
      "h\n",
      "i\n",
      "j\n",
      "k\n",
      "l\n",
      "m\n",
      "n\n",
      "o\n",
      "p\n",
      "q\n",
      "r\n",
      "s\n",
      "t\n",
      "u\n",
      "v\n",
      "w\n",
      "x\n",
      "y\n",
      "z\n",
      "0  ends\n",
      "1  ends\n"
     ]
    }
   ],
   "source": [
    "# for cat in xrange(10):\n",
    "#     if cat%2==1:\n",
    "#         continue\n",
    "#     global heap \n",
    "#     global fp\n",
    "files = [\"final_index/t.txt\", \"final_index/t1.txt\" ]\n",
    "fp = [open(fil) for fil in files]\n",
    "print fp\n",
    "for did in xrange(len(fp)):\n",
    "    pushin(did)\n",
    "if not heap:\n",
    "    fp = [f.close() for f in fp]\n",
    "    exit()\n",
    "\n",
    "# index = defaultdict(str)\n",
    "cur, plist, did = nextTok()\n",
    "pushin(did)\n",
    "\n",
    "nword, nplist, did = nextTok()\n",
    "pushin(did)\n",
    "\n",
    "final = open(\"final_index/ft.txt\", 'w')\n",
    "last = 'a'\n",
    "while heap:\n",
    "    if nword == cur:\n",
    "        plist += \"|\" + nplist\n",
    "    else:\n",
    "        final.write(cur.encode('ascii', 'ignore').decode('ascii') + \"|\" + plist + \"\\n\")\n",
    "#         index[cur] = plist\n",
    "        cur, plist = nword, nplist\n",
    "        if 'a' <= cur[0] <= 'z' and cur[0] != last:\n",
    "            last = cur[0]\n",
    "            print last\n",
    "#             for key in sorted(index):\n",
    "#                 cnt += 1\n",
    "\n",
    "#             index.clear()\n",
    "#             index = defaultdict(str)\n",
    "#             gc.collect()\n",
    "\n",
    "    nword, nplist, did = nextTok()\n",
    "    pushin(did)\n",
    "\n",
    "for f in fp:\n",
    "    f.close()\n",
    "final.close()\n",
    "# cat += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fil = open('invertedIndex.txt', 'a')\n",
    "# for key in sorted(index):\n",
    "#     cnt += 1\n",
    "#     fil.write(key.encode('ascii', 'ignore').decode('ascii') + \"|\" + index[key] + \"\\n\")\n",
    "# fil.close()\n",
    "def createSecIndex():\n",
    "    secIndPnt = open(\"final_index/secInverIndexE.txt\", \"w\")\n",
    "    lineCnt = 0\n",
    "    with open(\"final_index/e.txt\", \"r\") as fil:\n",
    "        item = fil.readline()\n",
    "        while item:\n",
    "            token = item.partition(\"|\")[0]\n",
    "            secIndPnt.write(token + \":\" + str(lineCnt)+\"\\n\")\n",
    "            lineCnt += len(item.encode('utf-8'))\n",
    "            item = fil.readline()\n",
    "            \n",
    "    secIndPnt.close()\n",
    "createSecIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp = open(\"final_index/b.txt\", \"r\")\n",
    "# secIndPnt = open(\"final_index/secInverIndex.txt\", \"r\")\n",
    "# for i in range(10):\n",
    "#     line = secIndPnt.readline()\n",
    "#     offset = int(line.split(\":\")[1].rstrip(\"\\n\"))\n",
    "#     print offset\n",
    "#     fp.seek(offset, 0)\n",
    "#     print fp.readline().partition(\"|\")[0]\n",
    "# fp.close()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
